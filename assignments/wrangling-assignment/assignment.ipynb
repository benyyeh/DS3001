{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec6fff7",
      "metadata": {},
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  - This paper discusses data cleaning, more specifically \"data tidying\". Data tidying entails cleaning data to adhere to a certain structure, allowing for a small set of cleaning tools, making it easier to develop tools and tidy the data itself.\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  - The intention of the \"tidy data standard\" is to bring consistency to data cleaning, making it easier by removing a lot of the additional costs of \"translation\" between different tools. \n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  - \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way\" means that all tidy datasets are clean, following the same structure, but messy datasets are messy in their own way and require differing approaches to clean them. \n",
        "  - The sentence, \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general\" says that distinguishing what elements of a dataset are observations and what are variables is generally easy, but coming up with a set definition for observation and variable that can be applied across all datasets is difficult. \n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  - Value: a datapoint, usually either numbers or strings.\n",
        "  - Variable: Set of values that measure the same underlying attribute across units.\n",
        "  - Observation: Set of values measured on the same unit across attribute. \n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  - \"Tidy Data\" is a standard of mapping the meaning of a dataset to its structure, adhering to 3 principles:\n",
        "    1. Each variable forms a column.\n",
        "    2. Each observation forms a row.\n",
        "    3. Each type of observational unit forms a table\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  - The 5 most common problems with messy datasets are:\n",
        "    - Column headers are values, not variable names.\n",
        "    - Multiple variables are stored in one column.\n",
        "    - Variables are stored in both rows and columns.\n",
        "    - Multiple types of observational units are stored in the same table.\n",
        "    - A single observational unit is stored in multiple tables\n",
        "  \n",
        "    The data in Table 4 is messy because the column headers are income ranges and should instead be variable values. \"Melting\" a dataset involves turning columns into rows. \n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  - Table 11 is messy because there is a column for each possible day in the month. Table 12 is clean because there is a row for each possible day and is molten because it turned the columns from table 11 into rows. \n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
        "  - The \"chicken-and-egg\" problem with focusing on tidy data is that tidy data is only as useful as the tools that work with tidy data and tidy tools are only useful with tidy data. Wickham's hope is that the tidy framework is built upon to develop even better tools and methodologies as a final solution to data wrangling. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "202c53e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Price data type before: object\n",
            "Price data type after: float64\n",
            "Missing values after converting to numeric: 181\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "airbnb_df = pd.read_csv('./data/airbnb_hw.csv')\n",
        "print(\"Price data type before:\", airbnb_df['Price'].dtypes)\n",
        "\n",
        "airbnb_df['Price'] = pd.to_numeric(airbnb_df['Price'], errors='coerce')\n",
        "airbnb_df['price_nan'] = airbnb_df['Price'].isnull()\n",
        "print(\"Price data type after:\", airbnb_df['Price'].dtypes)\n",
        "print(\"Missing values after converting to numeric:\", sum(airbnb_df['price_nan']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897685c7",
      "metadata": {},
      "source": [
        "I cleaned the `Price` variable by coverting it to numeric and coercing the errors. This caused 181 missing values. Further examining the data, it appears value that exceed 999 are formatted as a string with commas (e.g. \"1,000\"). The commas stop the value from being properly converted to numeric, causing a null value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b636375d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Price data type before: object\n",
            "Price data type after: int64\n",
            "Missing values after converting to numeric: 0\n"
          ]
        }
      ],
      "source": [
        "airbnb_df = pd.read_csv('./data/airbnb_hw.csv')\n",
        "print(\"Price data type before:\", airbnb_df['Price'].dtypes)\n",
        "\n",
        "airbnb_df['Price'] = pd.to_numeric(airbnb_df['Price'].replace({',':''}, regex = True), errors='coerce')\n",
        "airbnb_df['price_nan'] = airbnb_df['Price'].isnull()\n",
        "print(\"Price data type after:\", airbnb_df['Price'].dtypes)\n",
        "print(\"Missing values after converting to numeric:\", sum(airbnb_df['price_nan']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b843f5e",
      "metadata": {},
      "source": [
        "To get around this, I use the replace method to replace the occurences of ',' in the `Price` column. This allows the values over 999 to be properly convered to numeric and there are no 0 missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0b71a8c",
      "metadata": {},
      "source": [
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "d43de4f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type data type:  object\n",
            "Type value counts before:  Type\n",
            "Unprovoked             4716\n",
            "Provoked                593\n",
            "Invalid                 552\n",
            "Sea Disaster            239\n",
            "Watercraft              142\n",
            "Boat                    109\n",
            "Boating                  92\n",
            "Questionable             10\n",
            "Unconfirmed               1\n",
            "Unverified                1\n",
            "Under investigation       1\n",
            "Boatomg                   1\n",
            "Name: count, dtype: int64\n",
            "Type value counts after:  Type\n",
            "Unprovoked    4716\n",
            "Provoked       593\n",
            "Watercraft     583\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "sharks_df = pd.read_csv('./data/sharks.csv', low_memory=False)\n",
        "print(\"Type data type: \", sharks_df['Type'].dtype)\n",
        "print(\"Type value counts before: \", sharks_df['Type'].value_counts())\n",
        "\n",
        "dummy = sharks_df['Type']\n",
        "dummy = dummy.replace(['Sea Disaster', 'Boat', 'Boating','Boatomg'], 'Watercraft')\n",
        "dummy = dummy.replace(['Invalid', 'Questionable', 'Unconfirmed', 'Unverified', 'Under investigation'], np.nan)\n",
        "sharks_df['Type'] = dummy\n",
        "\n",
        "print(\"Type value counts after: \", sharks_df['Type'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb56eab4",
      "metadata": {},
      "source": [
        "Examining the `Type` variable's value counts, there appears to be 4 main categories, Provoked, Unprovoked, Watercraft related, and unknown. I began by replacing all watercraft related occurences with 'Watercraft'. I then removed all of the seemingly unknown values with null. After cleaning, ther are 4 values for `Type`, Unprovoked, Provoked, Watercraft, and nan. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c072d0",
      "metadata": {},
      "source": [
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "02ef9799",
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "df = pd.read_csv(url,low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "c8c148c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column values:  WhetherDefendantWasReleasedPretrial\n",
            "1    19154\n",
            "0     3801\n",
            "9       31\n",
            "Name: count, dtype: int64\n",
            "Column values:  WhetherDefendantWasReleasedPretrial\n",
            "1.0    19154\n",
            "0.0     3801\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Column values: \", df['WhetherDefendantWasReleasedPretrial'].value_counts())\n",
        "\n",
        "# replace unclear values with np.nan using dummy variable\n",
        "dummy = df['WhetherDefendantWasReleasedPretrial']\n",
        "dummy = dummy.replace(9, np.nan)\n",
        "dummy = df['WhetherDefendantWasReleasedPretrial'] = dummy\n",
        "print(\"Column values: \", df['WhetherDefendantWasReleasedPretrial'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12bb1d0b",
      "metadata": {},
      "source": [
        "Examing the `WhetherDefendantWasReleasedPretrial`, I noticed 3 values: 0 meaning no, 1 meaning yes, and 9 which doesn't have a clear meaning. Using a dummy variable, I replaced all the 9 values with np.nan. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eac956e",
      "metadata": {},
      "source": [
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "7e73e5fe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column values:  ImposedSentenceAllChargeInContactEvent\n",
            "                    9053\n",
            "0                   4953\n",
            "12                  1404\n",
            ".985626283367556    1051\n",
            "6                    809\n",
            "                    ... \n",
            "49.9712525667351       1\n",
            "57.0349075975359       1\n",
            "79.9260780287474       1\n",
            "42.1642710472279       1\n",
            "1.6570841889117        1\n",
            "Name: count, Length: 484, dtype: int64\n",
            "Column values:  SentenceTypeAllChargesAtConvictionInContactEvent\n",
            "4    8779\n",
            "0    8720\n",
            "1    4299\n",
            "2     914\n",
            "9     274\n",
            "Name: count, dtype: int64\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914     0    0\n",
            "True                                                 0     0    0  8779  274 \n",
            "\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914  8779    0\n",
            "True                                                 0     0    0     0  274 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Column values: \", df['ImposedSentenceAllChargeInContactEvent'].value_counts())\n",
        "print(\"Column values: \", df['SentenceTypeAllChargesAtConvictionInContactEvent'].value_counts())\n",
        "\n",
        "imposed = df['ImposedSentenceAllChargeInContactEvent']\n",
        "sentence_type = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "\n",
        "imposed = pd.to_numeric(imposed, errors='coerce')\n",
        "print(pd.crosstab(imposed.isnull(), sentence_type), '\\n')\n",
        "\n",
        "imposed = imposed.mask(sentence_type == 4, 0)\n",
        "imposed = imposed.mask(sentence_type == 9, np.nan)\n",
        "\n",
        "print(pd.crosstab(imposed.isnull(), sentence_type), '\\n')\n",
        "\n",
        "del imposed, sentence_type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558eb322",
      "metadata": {},
      "source": [
        "Examining the number of null values in `ImposedSentenceAllChargeInContactEvent` after converting to numeric, we see there are over 9,000 null values. Using the crosstab method, we see the majority of these null values come from the sentence type value 4 and 9. Sentence type 4 is where the charges were dropped, so I replaced all imposed sentence values with sentence type 4 with 0 because charges dropped means no sentence. I also replaced type 9 with null. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "2002env",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
